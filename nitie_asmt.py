# -*- coding: utf-8 -*-
"""NITIE_ASMT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UaGS1MWeE5MXRd0dlkGIe5r38J4Yicw_
"""

import pandas as pd
import numpy as np
# from utils import cal_Labels,cal_max_freq,create_dataframe
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import os

url = 'https://raw.githubusercontent.com/falaybeg/SparkStreaming-Network-Anomaly-Detection/master/TrainDf.csv'
traindata = pd.read_csv(url)

traindata

traindata.info()

traindata.describe()

traindata.corr()

import seaborn as sn
import matplotlib.pyplot as plt
corrMatrix = traindata.corr()

sn.set(rc = {'figure.figsize':(15,8)})
sn.heatmap(corrMatrix, annot=True)
plt.show()

from sklearn.feature_selection import SelectFromModel
y = traindata['status']
x = traindata.drop(['status'],axis=1)

"""## Univariate Selection"""

from numpy import set_printoptions
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
# feature extraction
test = SelectKBest(score_func=f_classif, k=4)
fit = test.fit(x, y)
# summarize scores
set_printoptions(precision=3)
print(fit.scores_)
features = fit.transform(x)
# summarize selected features
print(features[0:5,:])

features

"""## Recursive Feature Elimination"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings("ignore")
# feature extraction
model = LogisticRegression(solver='lbfgs')
rfe = RFE(model)
fit = rfe.fit(x, y)
print("Num Features: %d" % fit.n_features_)
print("Selected Features: %s" % fit.support_)
print("Feature Ranking: %s" % fit.ranking_)

traindata.columns

"""## Feature Importance"""

# feature extraction
from sklearn.ensemble import ExtraTreesClassifier
model = ExtraTreesClassifier()
model.fit(x, y)
print(model.feature_importances_)

IMP = []
for i in model.feature_importances_:
  if i>1.00e-02:
    IMP.append('YES')
  else:
    IMP.append('NO')

IMP

features = x.columns.tolist()
features[3]

for i in range(len(IMP)):
  if IMP[i] == 'NO':
    x.drop(features[i],axis=1,inplace=True)
    print(i,' :DROPPED')
  else:
    print(i,' :NOT DROPPED')
    pass

x

y.value_counts()

import seaborn as sns
sns.set_theme(style="darkgrid")
ax = sns.countplot(y, data=y)

dict = {'normal':0,'anomaly':1}
y=y.map(dict)

y

"""## Standard Scaling"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

"""## Model Training"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import classification_report
from sklearn.ensemble import GradientBoostingClassifier

import plotly.express as px  # for data visualization
import plotly.graph_objects as go # for data visualization

X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)

model = GradientBoostingClassifier(loss='deviance', #deviance or explonential
                               criterion='mse', #{‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’
                               learning_rate=0.1, # default 1.0
                               subsample=1.0, #default=1.0
                               random_state=0, # random state for reproducibility
                               max_features='sqrt', # number of random features to use sqrt(n_features)
                               min_samples_leaf=1000, # minimum no of observarions allowed in a leaf
                               max_depth=3, # maximum depth of the tree
                               n_estimators=500 # how many trees to build
                              )

clf = model.fit(X_train, y_train)

pred_labels_tr = model.predict(X_train)

pred_labels_te = model.predict(X_test)

# Basic info about the model
print('*************** Tree Summary ***************')
print('No. of classes: ', clf.n_classes_)
print('Classes: ', clf.classes_)
print('No. of features: ', clf.n_features_)
print('No. of Estimators: ', len(clf.estimators_))
print('--------------------------------------------------------')
print("")

print('*************** Evaluation on Test Data ***************')
score_te = model.score(X_test, y_test)
print('Accuracy Score: ', score_te)
# Look at classification report to evaluate the model
print(classification_report(y_test, pred_labels_te))
print('--------------------------------------------------------')
print("")

print('*************** Evaluation on Training Data ***************')
score_tr = model.score(X_train, y_train)
print('Accuracy Score: ', score_tr)
# Look at classification report to evaluate the model
print(classification_report(y_train, pred_labels_tr))
print('--------------------------------------------------------')

"""## Using Grid Search CV"""

from sklearn.model_selection import GridSearchCV
param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}
gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), 
param_grid = param_test2, scoring='roc_auc',n_jobs=4, cv=5)
gsearch2.fit(X_train, y_train)
gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_

gsearch2.best_score_

gsearch2.best_estimator_

gsearch2.best_params_

model = GradientBoostingClassifier(max_depth=15, max_features='sqrt',
                           min_samples_split=200, n_estimators=60,
                           random_state=10, subsample=0.8)

clf = model.fit(X_train, y_train)

pred_labels_tr = model.predict(X_train)
pred_labels_te = model.predict(X_test)

# Basic info about the model
print('*************** Tree Summary ***************')
print('No. of classes: ', clf.n_classes_)
print('Classes: ', clf.classes_)
print('No. of features: ', clf.n_features_)
print('No. of Estimators: ', len(clf.estimators_))
print('--------------------------------------------------------')
print("")

print('*************** Evaluation on Test Data ***************')
score_te = model.score(X_test, y_test)
print('Accuracy Score: ', score_te)
# Look at classification report to evaluate the model
print(classification_report(y_test, pred_labels_te))
print('--------------------------------------------------------')
print("")

print('*************** Evaluation on Training Data ***************')
score_tr = model.score(X_train, y_train)
print('Accuracy Score: ', score_tr)
# Look at classification report to evaluate the model
print(classification_report(y_train, pred_labels_tr))
print('--------------------------------------------------------')

